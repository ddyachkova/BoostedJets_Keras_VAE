{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Darya\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from Modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rcParams['figure.figsize'] = 3,3\n",
    "plot_img(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# of nodes in each layer \n",
    "n_nodes_inpl = 85*85   #encoder, input layer\n",
    "n_nodes_hl1  = 140  #encoder, first hidden layer \n",
    "n_nodes_hl2  = 140 #decoder, second hiddel layer \n",
    "n_nodes_outl = 85*85   #decoder, output layer\n",
    "\n",
    "# setting up the params \n",
    "shape = X_train.shape[1] * X_train.shape[2]\n",
    "learn_rate = 0.1   # the learning rate\n",
    "batch_size = 5000  # num of imgs in the same batch \n",
    "hm_epochs = 3000    # num of times to go through the entire dataset \n",
    "tot_images = X_train.shape[0] # num of images in total "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer, output_layer = get_output_layer(n_nodes_inpl, n_nodes_hl1, n_nodes_hl2, n_nodes_outl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Darya\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\training\\adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "output_true, meansq, optimizer = initialize(shape, output_layer, learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = initialize_sess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 3000 training loss: 1767783.625\n",
      "Epoch 1 / 3000 training loss: 217670.609375\n",
      "Epoch 2 / 3000 training loss: 110990.03515625\n",
      "Epoch 3 / 3000 training loss: 61363.337890625\n",
      "Epoch 4 / 3000 training loss: 36290.2822265625\n",
      "Epoch 5 / 3000 training loss: 22846.4580078125\n",
      "Epoch 6 / 3000 training loss: 15221.65087890625\n",
      "Epoch 7 / 3000 training loss: 10633.90185546875\n",
      "Epoch 8 / 3000 training loss: 7718.780029296875\n",
      "Epoch 9 / 3000 training loss: 5786.822265625\n",
      "Epoch 10 / 3000 training loss: 4465.697998046875\n",
      "Epoch 11 / 3000 training loss: 3525.53515625\n",
      "Epoch 12 / 3000 training loss: 2838.7012939453125\n",
      "Epoch 13 / 3000 training loss: 2324.45361328125\n",
      "Epoch 14 / 3000 training loss: 1932.3779907226562\n",
      "Epoch 15 / 3000 training loss: 1627.2338256835938\n",
      "Epoch 16 / 3000 training loss: 1385.49365234375\n",
      "Epoch 17 / 3000 training loss: 1190.9055786132812\n",
      "Epoch 18 / 3000 training loss: 1032.71923828125\n",
      "Epoch 19 / 3000 training loss: 902.7653198242188\n",
      "Epoch 20 / 3000 training loss: 794.8057861328125\n",
      "Epoch 21 / 3000 training loss: 704.3976440429688\n",
      "Epoch 22 / 3000 training loss: 627.6615600585938\n",
      "Epoch 23 / 3000 training loss: 562.6761169433594\n",
      "Epoch 24 / 3000 training loss: 506.9040069580078\n",
      "Epoch 25 / 3000 training loss: 458.38697814941406\n",
      "Epoch 26 / 3000 training loss: 416.4674072265625\n",
      "Epoch 27 / 3000 training loss: 379.8785095214844\n",
      "Epoch 28 / 3000 training loss: 347.83360290527344\n",
      "Epoch 29 / 3000 training loss: 319.64306640625\n",
      "Epoch 30 / 3000 training loss: 294.67401123046875\n",
      "Epoch 31 / 3000 training loss: 272.4688949584961\n",
      "Epoch 32 / 3000 training loss: 252.63439178466797\n",
      "Epoch 33 / 3000 training loss: 234.8561782836914\n",
      "Epoch 34 / 3000 training loss: 218.84835052490234\n",
      "Epoch 35 / 3000 training loss: 204.43038940429688\n",
      "Epoch 36 / 3000 training loss: 191.4163818359375\n",
      "Epoch 37 / 3000 training loss: 179.5894317626953\n",
      "Epoch 38 / 3000 training loss: 168.7949447631836\n",
      "Epoch 39 / 3000 training loss: 158.93224334716797\n",
      "Epoch 40 / 3000 training loss: 149.92640686035156\n",
      "Epoch 41 / 3000 training loss: 141.66885375976562\n",
      "Epoch 42 / 3000 training loss: 134.06372451782227\n",
      "Epoch 43 / 3000 training loss: 127.07326126098633\n",
      "Epoch 44 / 3000 training loss: 120.62201690673828\n",
      "Epoch 45 / 3000 training loss: 114.62413787841797\n",
      "Epoch 46 / 3000 training loss: 109.06816101074219\n",
      "Epoch 47 / 3000 training loss: 103.91365051269531\n",
      "Epoch 48 / 3000 training loss: 99.11559295654297\n",
      "Epoch 49 / 3000 training loss: 94.63858795166016\n",
      "Epoch 50 / 3000 training loss: 90.45429611206055\n",
      "Epoch 51 / 3000 training loss: 86.54731369018555\n",
      "Epoch 52 / 3000 training loss: 82.89287948608398\n",
      "Epoch 53 / 3000 training loss: 79.46931838989258\n",
      "Epoch 54 / 3000 training loss: 76.26265716552734\n",
      "Epoch 55 / 3000 training loss: 73.25053787231445\n",
      "Epoch 56 / 3000 training loss: 70.41863632202148\n",
      "Epoch 57 / 3000 training loss: 67.7578296661377\n",
      "Epoch 58 / 3000 training loss: 65.25298881530762\n",
      "Epoch 59 / 3000 training loss: 62.89023208618164\n",
      "Epoch 60 / 3000 training loss: 60.66009521484375\n",
      "Epoch 61 / 3000 training loss: 58.551700592041016\n",
      "Epoch 62 / 3000 training loss: 56.55940246582031\n",
      "Epoch 63 / 3000 training loss: 54.6731071472168\n",
      "Epoch 64 / 3000 training loss: 52.88486671447754\n",
      "Epoch 65 / 3000 training loss: 51.18933868408203\n",
      "Epoch 66 / 3000 training loss: 49.57968711853027\n",
      "Epoch 67 / 3000 training loss: 48.05022430419922\n",
      "Epoch 68 / 3000 training loss: 46.59625053405762\n",
      "Epoch 69 / 3000 training loss: 45.21221351623535\n",
      "Epoch 70 / 3000 training loss: 43.893022537231445\n",
      "Epoch 71 / 3000 training loss: 42.63518714904785\n",
      "Epoch 72 / 3000 training loss: 41.43634796142578\n",
      "Epoch 73 / 3000 training loss: 40.29071235656738\n",
      "Epoch 74 / 3000 training loss: 39.195966720581055\n",
      "Epoch 75 / 3000 training loss: 38.1441593170166\n",
      "Epoch 76 / 3000 training loss: 37.11646270751953\n",
      "Epoch 77 / 3000 training loss: 36.13288879394531\n",
      "Epoch 78 / 3000 training loss: 35.191091537475586\n",
      "Epoch 79 / 3000 training loss: 34.28973388671875\n",
      "Epoch 80 / 3000 training loss: 33.423370361328125\n",
      "Epoch 81 / 3000 training loss: 32.584075927734375\n",
      "Epoch 82 / 3000 training loss: 31.77866268157959\n",
      "Epoch 83 / 3000 training loss: 31.004968643188477\n",
      "Epoch 84 / 3000 training loss: 30.259336471557617\n",
      "Epoch 85 / 3000 training loss: 29.54249858856201\n",
      "Epoch 86 / 3000 training loss: 28.853278160095215\n",
      "Epoch 87 / 3000 training loss: 28.190199851989746\n",
      "Epoch 88 / 3000 training loss: 27.55207061767578\n",
      "Epoch 89 / 3000 training loss: 26.937535285949707\n",
      "Epoch 90 / 3000 training loss: 26.34469509124756\n",
      "Epoch 91 / 3000 training loss: 25.77345371246338\n",
      "Epoch 92 / 3000 training loss: 25.22206974029541\n",
      "Epoch 93 / 3000 training loss: 24.688840866088867\n",
      "Epoch 94 / 3000 training loss: 24.173710823059082\n",
      "Epoch 95 / 3000 training loss: 23.676267623901367\n",
      "Epoch 96 / 3000 training loss: 23.195656776428223\n",
      "Epoch 97 / 3000 training loss: 22.731261253356934\n",
      "Epoch 98 / 3000 training loss: 22.282438278198242\n",
      "Epoch 99 / 3000 training loss: 21.848551750183105\n",
      "Epoch 100 / 3000 training loss: 21.428966522216797\n",
      "Epoch 101 / 3000 training loss: 21.023877143859863\n",
      "Epoch 102 / 3000 training loss: 20.631342887878418\n",
      "Epoch 103 / 3000 training loss: 20.25046157836914\n",
      "Epoch 104 / 3000 training loss: 19.882469177246094\n",
      "Epoch 105 / 3000 training loss: 19.52492618560791\n",
      "Epoch 106 / 3000 training loss: 19.178446769714355\n",
      "Epoch 107 / 3000 training loss: 18.84243392944336\n",
      "Epoch 108 / 3000 training loss: 18.516671180725098\n",
      "Epoch 109 / 3000 training loss: 18.200756072998047\n",
      "Epoch 110 / 3000 training loss: 17.89404296875\n",
      "Epoch 111 / 3000 training loss: 17.596899032592773\n",
      "Epoch 112 / 3000 training loss: 17.308260917663574\n",
      "Epoch 113 / 3000 training loss: 17.027792930603027\n",
      "Epoch 114 / 3000 training loss: 16.75544834136963\n",
      "Epoch 115 / 3000 training loss: 16.490851879119873\n",
      "Epoch 116 / 3000 training loss: 16.23377275466919\n",
      "Epoch 117 / 3000 training loss: 15.98376750946045\n",
      "Epoch 118 / 3000 training loss: 15.740869045257568\n",
      "Epoch 119 / 3000 training loss: 15.504246234893799\n",
      "Epoch 120 / 3000 training loss: 15.274681091308594\n",
      "Epoch 121 / 3000 training loss: 15.050951957702637\n",
      "Epoch 122 / 3000 training loss: 14.833252906799316\n",
      "Epoch 123 / 3000 training loss: 14.621202945709229\n",
      "Epoch 124 / 3000 training loss: 14.414734840393066\n",
      "Epoch 125 / 3000 training loss: 14.213422298431396\n",
      "Epoch 126 / 3000 training loss: 14.017308235168457\n",
      "Epoch 127 / 3000 training loss: 13.826221466064453\n",
      "Epoch 128 / 3000 training loss: 13.639881610870361\n",
      "Epoch 129 / 3000 training loss: 13.458178043365479\n",
      "Epoch 130 / 3000 training loss: 13.280982971191406\n",
      "Epoch 131 / 3000 training loss: 13.108039379119873\n",
      "Epoch 132 / 3000 training loss: 12.939103603363037\n",
      "Epoch 133 / 3000 training loss: 12.774138450622559\n",
      "Epoch 134 / 3000 training loss: 12.613134860992432\n",
      "Epoch 135 / 3000 training loss: 12.455792903900146\n",
      "Epoch 136 / 3000 training loss: 12.302104949951172\n",
      "Epoch 137 / 3000 training loss: 12.151928901672363\n",
      "Epoch 138 / 3000 training loss: 12.004988193511963\n",
      "Epoch 139 / 3000 training loss: 11.861328125\n",
      "Epoch 140 / 3000 training loss: 11.720837593078613\n",
      "Epoch 141 / 3000 training loss: 11.583492755889893\n",
      "Epoch 142 / 3000 training loss: 11.449079513549805\n",
      "Epoch 143 / 3000 training loss: 11.317678928375244\n",
      "Epoch 144 / 3000 training loss: 11.188854694366455\n",
      "Epoch 145 / 3000 training loss: 11.062658786773682\n",
      "Epoch 146 / 3000 training loss: 10.939167976379395\n",
      "Epoch 147 / 3000 training loss: 10.818281173706055\n",
      "Epoch 148 / 3000 training loss: 10.69985294342041\n",
      "Epoch 149 / 3000 training loss: 10.583902835845947\n",
      "Epoch 150 / 3000 training loss: 10.470293045043945\n",
      "Epoch 151 / 3000 training loss: 10.358972072601318\n",
      "Epoch 152 / 3000 training loss: 10.249849796295166\n",
      "Epoch 153 / 3000 training loss: 10.142789363861084\n",
      "Epoch 154 / 3000 training loss: 10.037606716156006\n",
      "Epoch 155 / 3000 training loss: 9.934275150299072\n",
      "Epoch 156 / 3000 training loss: 9.832899570465088\n",
      "Epoch 157 / 3000 training loss: 9.733498096466064\n",
      "Epoch 158 / 3000 training loss: 9.635931968688965\n",
      "Epoch 159 / 3000 training loss: 9.540132999420166\n",
      "Epoch 160 / 3000 training loss: 9.446105480194092\n",
      "Epoch 161 / 3000 training loss: 9.353819847106934\n",
      "Epoch 162 / 3000 training loss: 9.26320219039917\n",
      "Epoch 163 / 3000 training loss: 9.174249172210693\n",
      "Epoch 164 / 3000 training loss: 9.0869140625\n",
      "Epoch 165 / 3000 training loss: 9.001142024993896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166 / 3000 training loss: 8.9168701171875\n",
      "Epoch 167 / 3000 training loss: 8.83413553237915\n",
      "Epoch 168 / 3000 training loss: 8.752867698669434\n",
      "Epoch 169 / 3000 training loss: 8.673052310943604\n",
      "Epoch 170 / 3000 training loss: 8.594640254974365\n",
      "Epoch 171 / 3000 training loss: 8.517611980438232\n",
      "Epoch 172 / 3000 training loss: 8.441940307617188\n",
      "Epoch 173 / 3000 training loss: 8.367568016052246\n",
      "Epoch 174 / 3000 training loss: 8.29446029663086\n",
      "Epoch 175 / 3000 training loss: 8.222591400146484\n",
      "Epoch 176 / 3000 training loss: 8.15195894241333\n",
      "Epoch 177 / 3000 training loss: 8.082509756088257\n",
      "Epoch 178 / 3000 training loss: 8.0142343044281\n",
      "Epoch 179 / 3000 training loss: 7.947106838226318\n",
      "Epoch 180 / 3000 training loss: 7.881088018417358\n",
      "Epoch 181 / 3000 training loss: 7.816147565841675\n",
      "Epoch 182 / 3000 training loss: 7.752265691757202\n",
      "Epoch 183 / 3000 training loss: 7.689423322677612\n",
      "Epoch 184 / 3000 training loss: 7.627580642700195\n",
      "Epoch 185 / 3000 training loss: 7.56672215461731\n",
      "Epoch 186 / 3000 training loss: 7.506831169128418\n",
      "Epoch 187 / 3000 training loss: 7.447889804840088\n",
      "Epoch 188 / 3000 training loss: 7.389857769012451\n",
      "Epoch 189 / 3000 training loss: 7.332751750946045\n",
      "Epoch 190 / 3000 training loss: 7.276514053344727\n",
      "Epoch 191 / 3000 training loss: 7.221159219741821\n",
      "Epoch 192 / 3000 training loss: 7.16663384437561\n",
      "Epoch 193 / 3000 training loss: 7.112950563430786\n",
      "Epoch 194 / 3000 training loss: 7.060113191604614\n",
      "Epoch 195 / 3000 training loss: 7.008080244064331\n",
      "Epoch 196 / 3000 training loss: 6.956814765930176\n",
      "Epoch 197 / 3000 training loss: 6.9063215255737305\n",
      "Epoch 198 / 3000 training loss: 6.856553316116333\n",
      "Epoch 199 / 3000 training loss: 6.807553291320801\n",
      "Epoch 200 / 3000 training loss: 6.759241342544556\n",
      "Epoch 201 / 3000 training loss: 6.7116475105285645\n",
      "Epoch 202 / 3000 training loss: 6.6647303104400635\n",
      "Epoch 203 / 3000 training loss: 6.618508338928223\n",
      "Epoch 204 / 3000 training loss: 6.572935342788696\n",
      "Epoch 205 / 3000 training loss: 6.52801251411438\n",
      "Epoch 206 / 3000 training loss: 6.483728408813477\n",
      "Epoch 207 / 3000 training loss: 6.440059661865234\n",
      "Epoch 208 / 3000 training loss: 6.396965742111206\n",
      "Epoch 209 / 3000 training loss: 6.354460954666138\n",
      "Epoch 210 / 3000 training loss: 6.312526702880859\n",
      "Epoch 211 / 3000 training loss: 6.2711875438690186\n",
      "Epoch 212 / 3000 training loss: 6.230408191680908\n",
      "Epoch 213 / 3000 training loss: 6.190180778503418\n",
      "Epoch 214 / 3000 training loss: 6.150493860244751\n",
      "Epoch 215 / 3000 training loss: 6.111334800720215\n",
      "Epoch 216 / 3000 training loss: 6.072689533233643\n",
      "Epoch 217 / 3000 training loss: 6.034559488296509\n",
      "Epoch 218 / 3000 training loss: 5.996941089630127\n",
      "Epoch 219 / 3000 training loss: 5.959820985794067\n",
      "Epoch 220 / 3000 training loss: 5.9231932163238525\n",
      "Epoch 221 / 3000 training loss: 5.887038230895996\n",
      "Epoch 222 / 3000 training loss: 5.851387023925781\n",
      "Epoch 223 / 3000 training loss: 5.816173553466797\n",
      "Epoch 224 / 3000 training loss: 5.781429052352905\n",
      "Epoch 225 / 3000 training loss: 5.747141599655151\n",
      "Epoch 226 / 3000 training loss: 5.713289022445679\n",
      "Epoch 227 / 3000 training loss: 5.679871559143066\n",
      "Epoch 228 / 3000 training loss: 5.646878719329834\n",
      "Epoch 229 / 3000 training loss: 5.614317417144775\n",
      "Epoch 230 / 3000 training loss: 5.582156419754028\n",
      "Epoch 231 / 3000 training loss: 5.550397872924805\n",
      "Epoch 232 / 3000 training loss: 5.51906418800354\n",
      "Epoch 233 / 3000 training loss: 5.488089084625244\n",
      "Epoch 234 / 3000 training loss: 5.457515716552734\n",
      "Epoch 235 / 3000 training loss: 5.427318096160889\n",
      "Epoch 236 / 3000 training loss: 5.397496461868286\n",
      "Epoch 237 / 3000 training loss: 5.368041038513184\n",
      "Epoch 238 / 3000 training loss: 5.338939189910889\n",
      "Epoch 239 / 3000 training loss: 5.310207366943359\n",
      "Epoch 240 / 3000 training loss: 5.281808376312256\n",
      "Epoch 241 / 3000 training loss: 5.253749132156372\n",
      "Epoch 242 / 3000 training loss: 5.226045608520508\n",
      "Epoch 243 / 3000 training loss: 5.1986470222473145\n",
      "Epoch 244 / 3000 training loss: 5.171578407287598\n",
      "Epoch 245 / 3000 training loss: 5.144838333129883\n",
      "Epoch 246 / 3000 training loss: 5.11840033531189\n",
      "Epoch 247 / 3000 training loss: 5.09226655960083\n",
      "Epoch 248 / 3000 training loss: 5.066457509994507\n",
      "Epoch 249 / 3000 training loss: 5.040931701660156\n",
      "Epoch 250 / 3000 training loss: 5.015693187713623\n",
      "Epoch 251 / 3000 training loss: 4.990756988525391\n",
      "Epoch 252 / 3000 training loss: 4.966104030609131\n",
      "Epoch 253 / 3000 training loss: 4.941730499267578\n",
      "Epoch 254 / 3000 training loss: 4.917642116546631\n",
      "Epoch 255 / 3000 training loss: 4.893821477890015\n",
      "Epoch 256 / 3000 training loss: 4.870274305343628\n",
      "Epoch 257 / 3000 training loss: 4.846995115280151\n",
      "Epoch 258 / 3000 training loss: 4.823972463607788\n",
      "Epoch 259 / 3000 training loss: 4.801202297210693\n",
      "Epoch 260 / 3000 training loss: 4.7786877155303955\n",
      "Epoch 261 / 3000 training loss: 4.756421327590942\n",
      "Epoch 262 / 3000 training loss: 4.73439884185791\n",
      "Epoch 263 / 3000 training loss: 4.712617874145508\n",
      "Epoch 264 / 3000 training loss: 4.691077709197998\n",
      "Epoch 265 / 3000 training loss: 4.669771194458008\n",
      "Epoch 266 / 3000 training loss: 4.648702383041382\n",
      "Epoch 267 / 3000 training loss: 4.627859592437744\n",
      "Epoch 268 / 3000 training loss: 4.607235431671143\n",
      "Epoch 269 / 3000 training loss: 4.586852312088013\n",
      "Epoch 270 / 3000 training loss: 4.566675424575806\n",
      "Epoch 271 / 3000 training loss: 4.5467143058776855\n",
      "Epoch 272 / 3000 training loss: 4.526974201202393\n",
      "Epoch 273 / 3000 training loss: 4.507429838180542\n",
      "Epoch 274 / 3000 training loss: 4.488088369369507\n",
      "Epoch 275 / 3000 training loss: 4.468959093093872\n",
      "Epoch 276 / 3000 training loss: 4.450016975402832\n",
      "Epoch 277 / 3000 training loss: 4.431269645690918\n",
      "Epoch 278 / 3000 training loss: 4.412725448608398\n",
      "Epoch 279 / 3000 training loss: 4.3943610191345215\n",
      "Epoch 280 / 3000 training loss: 4.376180410385132\n",
      "Epoch 281 / 3000 training loss: 4.358190059661865\n",
      "Epoch 282 / 3000 training loss: 4.340381860733032\n",
      "Epoch 283 / 3000 training loss: 4.322747230529785\n",
      "Epoch 284 / 3000 training loss: 4.3052942752838135\n",
      "Epoch 285 / 3000 training loss: 4.2880167961120605\n",
      "Epoch 286 / 3000 training loss: 4.270909547805786\n",
      "Epoch 287 / 3000 training loss: 4.253976583480835\n",
      "Epoch 288 / 3000 training loss: 4.2372047901153564\n",
      "Epoch 289 / 3000 training loss: 4.220600008964539\n",
      "Epoch 290 / 3000 training loss: 4.204157710075378\n",
      "Epoch 291 / 3000 training loss: 4.187881231307983\n",
      "Epoch 292 / 3000 training loss: 4.171764016151428\n",
      "Epoch 293 / 3000 training loss: 4.155805587768555\n",
      "Epoch 294 / 3000 training loss: 4.1400076150894165\n",
      "Epoch 295 / 3000 training loss: 4.124355435371399\n",
      "Epoch 296 / 3000 training loss: 4.108860373497009\n",
      "Epoch 297 / 3000 training loss: 4.093519330024719\n",
      "Epoch 298 / 3000 training loss: 4.07831609249115\n",
      "Epoch 299 / 3000 training loss: 4.063265442848206\n",
      "Epoch 300 / 3000 training loss: 4.048365473747253\n",
      "Epoch 301 / 3000 training loss: 4.033596038818359\n",
      "Epoch 302 / 3000 training loss: 4.018968105316162\n",
      "Epoch 303 / 3000 training loss: 4.0044926404953\n",
      "Epoch 304 / 3000 training loss: 3.9901411533355713\n",
      "Epoch 305 / 3000 training loss: 3.975926995277405\n",
      "Epoch 306 / 3000 training loss: 3.961851716041565\n",
      "Epoch 307 / 3000 training loss: 3.9479074478149414\n",
      "Epoch 308 / 3000 training loss: 3.9340851306915283\n",
      "Epoch 309 / 3000 training loss: 3.9203985929489136\n",
      "Epoch 310 / 3000 training loss: 3.906842589378357\n",
      "Epoch 311 / 3000 training loss: 3.893402934074402\n",
      "Epoch 312 / 3000 training loss: 3.8800872564315796\n",
      "Epoch 313 / 3000 training loss: 3.866907000541687\n",
      "Epoch 314 / 3000 training loss: 3.853837013244629\n",
      "Epoch 315 / 3000 training loss: 3.8408870697021484\n",
      "Epoch 316 / 3000 training loss: 3.8280545473098755\n",
      "Epoch 317 / 3000 training loss: 3.8153414726257324\n",
      "Epoch 318 / 3000 training loss: 3.802740693092346\n",
      "Epoch 319 / 3000 training loss: 3.790248155593872\n",
      "Epoch 320 / 3000 training loss: 3.7778791189193726\n",
      "Epoch 321 / 3000 training loss: 3.7654786109924316\n",
      "Epoch 322 / 3000 training loss: 3.7531944513320923\n",
      "Epoch 323 / 3000 training loss: 3.7410134077072144\n",
      "Epoch 324 / 3000 training loss: 3.7289446592330933\n",
      "Epoch 325 / 3000 training loss: 3.7169758081436157\n",
      "Epoch 326 / 3000 training loss: 3.705109715461731\n",
      "Epoch 327 / 3000 training loss: 3.693355441093445\n",
      "Epoch 328 / 3000 training loss: 3.6816967725753784\n",
      "Epoch 329 / 3000 training loss: 3.670137882232666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 330 / 3000 training loss: 3.65868079662323\n",
      "Epoch 331 / 3000 training loss: 3.6473238468170166\n",
      "Epoch 332 / 3000 training loss: 3.636056423187256\n",
      "Epoch 333 / 3000 training loss: 3.624884009361267\n",
      "Epoch 334 / 3000 training loss: 3.6138203144073486\n",
      "Epoch 335 / 3000 training loss: 3.6028398275375366\n",
      "Epoch 336 / 3000 training loss: 3.5919541120529175\n",
      "Epoch 337 / 3000 training loss: 3.581165075302124\n",
      "Epoch 338 / 3000 training loss: 3.570468783378601\n",
      "Epoch 339 / 3000 training loss: 3.559859275817871\n",
      "Epoch 340 / 3000 training loss: 3.549341559410095\n",
      "Epoch 341 / 3000 training loss: 3.5389180183410645\n",
      "Epoch 342 / 3000 training loss: 3.528576135635376\n",
      "Epoch 343 / 3000 training loss: 3.5183173418045044\n",
      "Epoch 344 / 3000 training loss: 3.5081557035446167\n",
      "Epoch 345 / 3000 training loss: 3.4980759620666504\n",
      "Epoch 346 / 3000 training loss: 3.488074779510498\n",
      "Epoch 347 / 3000 training loss: 3.4781585931777954\n",
      "Epoch 348 / 3000 training loss: 3.4683293104171753\n",
      "Epoch 349 / 3000 training loss: 3.458577513694763\n",
      "Epoch 350 / 3000 training loss: 3.448903441429138\n",
      "Epoch 351 / 3000 training loss: 3.4393149614334106\n",
      "Epoch 352 / 3000 training loss: 3.429801821708679\n",
      "Epoch 353 / 3000 training loss: 3.420361638069153\n",
      "Epoch 354 / 3000 training loss: 3.4109880924224854\n",
      "Epoch 355 / 3000 training loss: 3.401690363883972\n",
      "Epoch 356 / 3000 training loss: 3.392462372779846\n",
      "Epoch 357 / 3000 training loss: 3.383316159248352\n",
      "Epoch 358 / 3000 training loss: 3.37423837184906\n",
      "Epoch 359 / 3000 training loss: 3.365237236022949\n",
      "Epoch 360 / 3000 training loss: 3.3563075065612793\n",
      "Epoch 361 / 3000 training loss: 3.3474485874176025\n",
      "Epoch 362 / 3000 training loss: 3.338658571243286\n",
      "Epoch 363 / 3000 training loss: 3.329940676689148\n",
      "Epoch 364 / 3000 training loss: 3.3212947845458984\n",
      "Epoch 365 / 3000 training loss: 3.3127121925354004\n",
      "Epoch 366 / 3000 training loss: 3.3041986227035522\n",
      "Epoch 367 / 3000 training loss: 3.295756459236145\n",
      "Epoch 368 / 3000 training loss: 3.2873730659484863\n",
      "Epoch 369 / 3000 training loss: 3.2790576219558716\n",
      "Epoch 370 / 3000 training loss: 3.2708128690719604\n",
      "Epoch 371 / 3000 training loss: 3.26262629032135\n",
      "Epoch 372 / 3000 training loss: 3.254501223564148\n",
      "Epoch 373 / 3000 training loss: 3.2464475631713867\n",
      "Epoch 374 / 3000 training loss: 3.238449811935425\n",
      "Epoch 375 / 3000 training loss: 3.2305103540420532\n",
      "Epoch 376 / 3000 training loss: 3.2226369380950928\n",
      "Epoch 377 / 3000 training loss: 3.214816451072693\n",
      "Epoch 378 / 3000 training loss: 3.2070555686950684\n",
      "Epoch 379 / 3000 training loss: 3.199355363845825\n",
      "Epoch 380 / 3000 training loss: 3.1917141675949097\n",
      "Epoch 381 / 3000 training loss: 3.184126377105713\n",
      "Epoch 382 / 3000 training loss: 3.17659592628479\n",
      "Epoch 383 / 3000 training loss: 3.1691256761550903\n",
      "Epoch 384 / 3000 training loss: 3.161704897880554\n",
      "Epoch 385 / 3000 training loss: 3.154341697692871\n",
      "Epoch 386 / 3000 training loss: 3.147036910057068\n",
      "Epoch 387 / 3000 training loss: 3.1397827863693237\n",
      "Epoch 388 / 3000 training loss: 3.132581114768982\n",
      "Epoch 389 / 3000 training loss: 3.1254382133483887\n",
      "Epoch 390 / 3000 training loss: 3.118343234062195\n",
      "Epoch 391 / 3000 training loss: 3.111298084259033\n",
      "Epoch 392 / 3000 training loss: 3.1043097972869873\n",
      "Epoch 393 / 3000 training loss: 3.0973682403564453\n",
      "Epoch 394 / 3000 training loss: 3.090471625328064\n",
      "Epoch 395 / 3000 training loss: 3.083631753921509\n",
      "Epoch 396 / 3000 training loss: 3.0768344402313232\n",
      "Epoch 397 / 3000 training loss: 3.070088505744934\n",
      "Epoch 398 / 3000 training loss: 3.063393235206604\n",
      "Epoch 399 / 3000 training loss: 3.0567445755004883\n",
      "Epoch 400 / 3000 training loss: 3.050140142440796\n",
      "Epoch 401 / 3000 training loss: 3.04358971118927\n",
      "Epoch 402 / 3000 training loss: 3.0370815992355347\n",
      "Epoch 403 / 3000 training loss: 3.030617117881775\n",
      "Epoch 404 / 3000 training loss: 3.024206280708313\n",
      "Epoch 405 / 3000 training loss: 3.0178338289260864\n",
      "Epoch 406 / 3000 training loss: 3.0115044116973877\n",
      "Epoch 407 / 3000 training loss: 3.0052188634872437\n",
      "Epoch 408 / 3000 training loss: 2.9989770650863647\n",
      "Epoch 409 / 3000 training loss: 2.99277663230896\n",
      "Epoch 410 / 3000 training loss: 2.9866247177124023\n",
      "Epoch 411 / 3000 training loss: 2.9805169105529785\n",
      "Epoch 412 / 3000 training loss: 2.9744482040405273\n",
      "Epoch 413 / 3000 training loss: 2.9684234857559204\n",
      "Epoch 414 / 3000 training loss: 2.9624418020248413\n",
      "Epoch 415 / 3000 training loss: 2.9564974308013916\n",
      "Epoch 416 / 3000 training loss: 2.9505966901779175\n",
      "Epoch 417 / 3000 training loss: 2.9447338581085205\n",
      "Epoch 418 / 3000 training loss: 2.938910484313965\n",
      "Epoch 419 / 3000 training loss: 2.933130979537964\n",
      "Epoch 420 / 3000 training loss: 2.927388310432434\n",
      "Epoch 421 / 3000 training loss: 2.921681523323059\n",
      "Epoch 422 / 3000 training loss: 2.9160139560699463\n",
      "Epoch 423 / 3000 training loss: 2.910382628440857\n",
      "Epoch 424 / 3000 training loss: 2.904784917831421\n",
      "Epoch 425 / 3000 training loss: 2.8992278575897217\n",
      "Epoch 426 / 3000 training loss: 2.8937071561813354\n",
      "Epoch 427 / 3000 training loss: 2.8882206678390503\n",
      "Epoch 428 / 3000 training loss: 2.882773518562317\n",
      "Epoch 429 / 3000 training loss: 2.8773618936538696\n",
      "Epoch 430 / 3000 training loss: 2.8719828128814697\n",
      "Epoch 431 / 3000 training loss: 2.8666404485702515\n",
      "Epoch 432 / 3000 training loss: 2.8613370656967163\n",
      "Epoch 433 / 3000 training loss: 2.85606586933136\n",
      "Epoch 434 / 3000 training loss: 2.8508280515670776\n",
      "Epoch 435 / 3000 training loss: 2.845625162124634\n",
      "Epoch 436 / 3000 training loss: 2.8404531478881836\n",
      "Epoch 437 / 3000 training loss: 2.8353151082992554\n",
      "Epoch 438 / 3000 training loss: 2.8302111625671387\n",
      "Epoch 439 / 3000 training loss: 2.825133204460144\n",
      "Epoch 440 / 3000 training loss: 2.8200896978378296\n",
      "Epoch 441 / 3000 training loss: 2.8150815963745117\n",
      "Epoch 442 / 3000 training loss: 2.8101035356521606\n",
      "Epoch 443 / 3000 training loss: 2.8051501512527466\n",
      "Epoch 444 / 3000 training loss: 2.8002331256866455\n",
      "Epoch 445 / 3000 training loss: 2.7953414916992188\n",
      "Epoch 446 / 3000 training loss: 2.7904818058013916\n",
      "Epoch 447 / 3000 training loss: 2.7856602668762207\n",
      "Epoch 448 / 3000 training loss: 2.7808642387390137\n",
      "Epoch 449 / 3000 training loss: 2.776094436645508\n",
      "Epoch 450 / 3000 training loss: 2.7713630199432373\n",
      "Epoch 451 / 3000 training loss: 2.7666558027267456\n",
      "Epoch 452 / 3000 training loss: 2.76197612285614\n",
      "Epoch 453 / 3000 training loss: 2.757319688796997\n"
     ]
    }
   ],
   "source": [
    "epoch_loss_arr = []\n",
    "with tf.device(tf.DeviceSpec(device_type=\"GPU\", device_index=0)):\n",
    "    for epoch in range(hm_epochs):\n",
    "        epoch_loss = 0    # initializing error as 0\n",
    "        num_iter = int(tot_images/batch_size)\n",
    "        for i in range(num_iter):\n",
    "            epoch_x = X_train[i*batch_size : (i+1)*batch_size].reshape(batch_size, 85*85)\n",
    "            _, c = sess.run([optimizer, meansq], feed_dict={input_layer: epoch_x, output_true: epoch_x})\n",
    "            epoch_loss += c\n",
    "        epoch_loss_arr = np.append(epoch_loss_arr, epoch_loss)\n",
    "        print('Epoch', epoch, '/', hm_epochs, 'training loss:',epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5,12):\n",
    "    rcParams['figure.figsize'] = 3,3\n",
    "    plot_img(X_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5,12):\n",
    "    outpt = sess.run(output_layer, feed_dict={input_layer:[X_test[i].reshape(shape)]}).reshape(X_train.shape[1], X_train.shape[2])\n",
    "    plot_img(outpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Conv2D, Conv2DTranspose, MaxPooling2D, Dropout, UpSampling2D\n",
    "from keras.layers import Flatten, Activation\n",
    "from keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape1, img_shape2, num_channels = X_train.shape[1],  X_train.shape[2], 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = encoder_decoder(img_shape1, img_shape2, num_channels)\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(X_train, X_train, epochs=num_epochs, batch_size=batch_size, shuffle=True, validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = autoencoder.predict(X_test_noisy.reshape(X_test_noisy.shape[0], 68, 60, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(decoded[3])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
